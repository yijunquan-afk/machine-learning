# 机器学习——04朴素贝叶斯

## 参考资料

1. [AIlearning](https://ailearning.apachecn.org/#/docs/ml/2)
2. [Machine-Learning-in-Action](https://github.com/TeFuirnever/Machine-Learning-in-Action)
3. 庞善民.西安交通大学机器学习导论2022春PPT

更多原理请参考本人另一篇博客：[[机器学习导论]——第六课——贝叶斯分类器](https://blog.csdn.net/weixin_47692652/article/details/124309268)

## 一、知识准备

### 贝叶斯公式

针对两个随机变量，联合概率分布具有两种分解形式
$$
P(x,y)=P(x|y)P(y)=P(y|x)P(x)
$$
因此，利用上式得到贝叶斯公式

$$
P(c|x)=\frac{P(x|c)P(c)}{P(x)}
$$

通过贝叶斯公式得到贝叶斯决策理论基本思想：

1️⃣ 已知**类条件概率**密度参数表达式$P(x|c)$和**先验概率**$P(c)$

2️⃣ 利用贝叶斯公式转换成**后验概率**

3️⃣ 根据后验概率大小进行决策分类

> **先验概率（prior probability）：**指根据以往经验和分析。在实验或采样前就可以得到的概率。
>
> **后验概率（posterior probability）：**指某件事已经发生，想要计算这件事发生的原因是由某个因素引起的概率。
>
> **类条件概率密度**是，假定x是一个连续随机变量，其分布取决于类别状态，表示成p(x|ω)的形式，这就是“类条件概率密度”函数，即**类别状态为ω时的x的**[概率密度函数](https://baike.baidu.com/item/概率密度函数/5021996)（有时也称为状态条件概率密度）。

### 贝叶斯决策基础

贝叶斯分类决策利用概率对数据进行建模，从而基于**贝叶斯定理**给出分类预测的不确定性

将特征向量$x=(x_1,...,x_p)^T$类别标签𝑐𝑗作为**随机变量**

给定样本𝒙,基于条件（后验）概率$P(c_i|x)$计算将样本𝒙分类为𝑐𝑖所产生的期望损失
$$
R(c_i|\pmb x)= \sum_{j=1}^L\lambda_{ij}P(c_j|\pmb x)
$$
其中,$\lambda_{ij}$是将一个真实标记为$c_j$的样本误分类为$c_i$所产生的损失

贝叶斯判定准则要求 <mark>期望损失达到最小 </mark>。
$$
h^*(x)=\text{argmin}\ R(c|x)
$$
称$h^*(x)$为贝叶斯最优分类器

进一步，若目标是最小化分类错误率，则

![image-20220321171559820](https://img-blog.csdnimg.cn/img_convert/f44f6a3401e67dedb196f3fb6aa7b142.png)

期望损失可以写成
$$
R(c_i|x)=P(c_1|x)+...+P(c_{i-1}|x)+P(c_{i+1}|x)+...+P(c_{L}|x)=1-P(c_i|x)
$$
于是，对每个样本，选择使**后验概率最大的类别标记**
$$
c_{MAP}=\underset{c_j\in C}{\text{argmax}}\ P(c_j|x_1,x_2,...,x_p)
$$

## 二、MAP分类准则

估计后验概率𝑃(𝑐|𝑥)的方法主要有两种策略：

1️⃣ **判别式模型**：通过对𝑃(𝑐|𝑥)直接建模预测
![在这里插入图片描述](https://img-blog.csdnimg.cn/d47ac1f949fc4706adee757efaaeccd8.png)

逻辑回归：
$$
P(c|x;\theta)=(f_\theta(x))^c(1-f_\theta(x))^{1-c}\\
c\in{0,1}\ f_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
**直接对条件概率建模，不关心背后的数据分布**$P(x,c)$

2️⃣ **生成式模型**：使用贝叶斯推理预测，即假定类条件概率具有某种确定的概率分布

![image-20220321171808451](https://img-blog.csdnimg.cn/img_convert/39af1b5b73a151f71f9805fc6ec3040c.png)

先对联合概率分布𝑃(𝑥,𝑐)建模，再通过贝叶斯公式计算后验概率𝑃(𝑐|𝑥)

![image-20220420162438848](https://img-blog.csdnimg.cn/img_convert/1a7db8476e399864e298c90ddcd2cf0f.png)

先对联合概率分布 𝑃(𝑥, 𝑐) 建模，再通过贝叶斯公式计算后验概率 𝑃( 𝑐| 𝑥)

GANs：适用于娱乐行业

## 三、贝叶斯分类算法

### 一般生成式贝叶斯分类器

#### 公式说明

基于贝叶斯公式估计后验概率：

![image-20220321172252379](https://img-blog.csdnimg.cn/img_convert/00d8c5a949c2793556f554caa3b48e2c.png)

使用最大后验概率准则给出类标签

![image-20220321172306382](https://img-blog.csdnimg.cn/img_convert/29b291369166c466894ecad453b5ec27.png)

#### 举例说明

例子：今天我们可以打网球吗？

训练示例集如下

![image-20220420164325967](https://img-blog.csdnimg.cn/img_convert/3e4e52f5ae2de131ff9769662366175f.png)

如果给一个新样本：X = (天气  = 晴, 气温  = 冷, 湿度  = 高, 风  = 有)，想知道是否可以打网球？

依据大数定律，利用样本出现频率估计先验概率

> 在试验不变的条件下，重复试验多次，随机事件的频率近似于它的概率。偶然中包含着某种必然。

![image-20220420164601250](https://img-blog.csdnimg.cn/img_convert/2c12d30feaeda2732c5d5635f95f5ef4.png)

其次，估计类条件概率

![image-20220420164825761](https://img-blog.csdnimg.cn/img_convert/6bef4d5570deab813a69ccab0ef28e52.png)

根据样本出现频率估计条件概率。但由于样本数远小于随机向量的可能取值数目，估计值通常不可靠

> 例如：$P(多云，热，高，无|Yes)=\frac{1}{9}\qquad P(多云，热，高，无|Yes)=\frac{0}{5}$
>
> <mark>未观测到≠出现概率为0</mark>

**训练过程**

**获得先验概率**
$$
P(X=Yes)=\frac{9}{14}\qquad P(C=No)=\frac{5}{14}
$$
**得到类条件概率表**

![image-20220420165625019](https://img-blog.csdnimg.cn/img_convert/23c6905c9a4e7887fdb8f03c86a3f8b6.png)

**测试阶段**

给定测试样本：X = (天气 = 晴, 气温 = 冷, 湿度 = 高, 风 = 有)， 通过查找条件概率表，可以得到
$$
P(X|Yes)P(C=Yes)=\frac{0}{9}\times =\frac{9}{14}=0\\
P(X|No)P(C=No)=\frac{0}{5}\times =\frac{5}{14}=0\\
$$
由此，基于贝叶斯公式:
$$
P(Yes|X)=0\qquad P(No|X)=0
$$
<mark>打和不打都是0，效果不好！</mark>

### 朴素贝叶斯分类器

#### 公式说明

朴素贝叶斯分类：对已知类别，**假设所有属性相互独立**（属性条件独立性假设）

![image-20220321173421760](https://img-blog.csdnimg.cn/img_convert/20df58d7296002948cb34deb6e4e3fe0.png)

因此朴素贝叶斯的分类公式为：

![image-20220321173455253](https://img-blog.csdnimg.cn/img_convert/4f12b1a2e25d71b380decff26d090fff.png)

在训练时，朴素贝叶斯为每个属性估计条件概率$P(x_i|c_j)$

假设样本的𝑝个属性都有𝑑种可能取值，则共需要估计𝑑𝑝个条件概率

> 朴素贝叶斯的朴素体现在其对各个条件的独立性假设上，加上独立假设后，大大减少了参数的假设空间：从$d^p$降到了𝑑𝑝。

#### 举例说明

例子：今天我们可以打网球吗？

训练示例集如下

![image-20220420164325967](https://img-blog.csdnimg.cn/img_convert/3e4e52f5ae2de131ff9769662366175f.png)

如果给一个新样本：X = (天气  = 晴, 气温  = 冷, 湿度  = 高, 风  = 有)，想知道是否可以打网球？

**需要估计**

> 先验$P(C=c_j)$
>
> 每个属性的条件概率$P(x_i|c_j)$

使用样本出现的概率
$$
\hat{P}(c_j)=\frac{N(C=c_j)}{N}\\
\hat{P}(x_i|c_j)=\frac{N(X_i=x_i,C=c_j)}{N(C=c_j)}
$$
对于打网球问题，有

先验概率：
$$
P(C=Yes)=9/14\qquad P(C=No)=5/14
$$
条件概率$P(X_i|C_j)$

![image-20220420203005417](https://img-blog.csdnimg.cn/img_convert/9695cbfb0c5aedeecd22c0533ab011bb.png)

测试步骤

> :one: 给定新样本：X = (天气 = 晴, 气温 = 冷, 湿度 = 高, 风 = 有)
>
> :two: 查先验和条件概率表
> $$
> P(C=Yes)=9/14\qquad P(C=No)=5/14
> $$
> ![image-20220420203149604](https://img-blog.csdnimg.cn/img_convert/8a952e189cc59ec370324b67fb6e43a4.png)
>
> :three: 计算后验概率
>
> ![image-20220420203315992](https://img-blog.csdnimg.cn/img_convert/600b38451841f9f3261981d7bfd72659.png)
>
> :four: 因为P(Yes|x)<P(No|x)，根据MAP准则，预测为"不打网球"

#### 避免0概率问题

若某个属性值在训练集中没有与某个类同时出现过，则基于频率的概率估计将为零

不合理：仅仅因为事件之前没有发生过，并不意味着它不会发生，为避免这一情况，**需要对概率值进行平滑**

解决方案：**使用拉普拉斯校正**
$$
\hat{P}(c_j)=\frac{N(C=c_j)+1}{N+|C|}\\
\hat{P}(x_i|c_j)=\frac{N(X_i=x_i,C=c_j)+1}{N(C=c_j)+|X_i|}\\
|C|\rightarrow 类的个数\qquad |X_i|\rightarrow 属性的取值数目
$$
例如：

![image-20220420203712736](https://img-blog.csdnimg.cn/img_convert/4b4aab80874123a11c88922ee3eecf28.png)
$$
P(X_1=多云|C=No)=\frac{0+1}{5+3}=\frac{1}{8}
$$
避免了因训练样本不足而导致的概率估值为0的问题。

## 四、朴素贝叶斯应用场景

机器学习的一个重要应用就是**文档的自动分类**。

在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。

朴素贝叶斯是上面介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。下面我们会进行一些朴素贝叶斯分类的实践项目。

### 工作原理

- 提取所有文档中的词条并进行去重 

- 获取文档的所有类别 

- 计算每个类别中的文档数目 

- 对每篇训练文档:     

​	对每个类别:         

​		如果词条出现在文档中-->增加该词条的计数值（for循环或者矩阵相加）        

​		增加所有词条的计数值（此类别下词条总数） 

- 对每个类别:     

​	对每个词条:         

​		将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)） 

- 返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)）

### 开发流程

1. 
   收集数据: 可以使用任何方法。

2. 准备数据: 需要数值型或者布尔型数据。

3. 分析数据: 有大量特征时，绘制特征作用不大，此时使用直方图效果更好。

4. 训练算法: 计算不同的独立特征的条件概率。

5. 测试算法: 计算错误率。

6. 使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。


### 算法特点

- 优点: 在数据较少的情况下仍然有效，可以处理多类别问题。 
- 缺点: 对于输入数据的准备方式较为敏感。 
- 适用数据类型: 标称型数据。

## 五、项目案例1: 屏蔽社区留言板的侮辱性言论

### 项目概述

构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。

### 开发流程

- 收集数据: 可以使用任何方法 
- 准备数据: 从文本中构建词向量 
- 分析数据: 检查词条确保解析的正确性 
- 训练算法: 从词向量计算概率 
- 测试算法: 根据现实情况修改分类器 
- 使用算法: 对社区留言板言论进行分类

### 收集数据

使用简单的数据集：

```python
def loadDataSet():
    """
    创建数据集
    :return: 单词列表postingList, 所属类别classVec
    """
    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    classVec = [0, 1, 0, 1, 0, 1]  # 1为正例
    return postingList, classVec    
```

### 准备数据

从文本中构建词向量
$$
p(c_i|\pmb w)=\frac{p(\pmb w|c_i)p(c_i)}{p(\pmb w)}
$$
![image-20221217120251145](https://note-image-1307786938.cos.ap-beijing.myqcloud.com/typora/image-20221217120251145.png)